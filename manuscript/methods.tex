% methods.tex
% CANDI: Confidence-Aware Neural Denoising Imputer for Epigenomic Data

\section{Methods}

\subsection{Data Collection and Processing}

We trained and evaluated CANDI using two datasets.

\subsubsection{ENCODE Imputation Challenge Dataset}

We used data from the ENCODE Imputation Challenge (EIC), featuring 35 distinct assays measured across 50 biosamples. For each experiment, we obtained aligned sequencing reads (BAM files), signal $p$-values (BigWig files), peak calls (BED files), and experimental covariates, including sequencing depth, sequencing platform, read length, and run type (single- or paired-end). In the original ENCODE Imputation Challenge, the dataset contained only BigWig files of signal $p$-values. However, since we use read counts as input to our model, we obtained raw read files (BAM) corresponding to the same experiments and biosamples in the EIC. We followed the original train-validation-test split proposed by the EIC to ensure comparability with previous benchmarks.

\subsubsection{Extended ENCODE Dataset}

To have a more extensive dataset, we systematically collected data for all biosamples in the ENCODE database that contained at least one experiment from the 35 assays of interest. This initial collection yielded 3,064 biosamples, forming a sparse $3{,}064 \times 35$ experiment availability matrix. To address this sparsity while maintaining biological relevance, we implemented a cell type-based merging strategy. This merging strategy significantly improved data density, increasing the median number of available assays per sample from 1 (per biosample) to 6 (per merged cell type). The final processed dataset consists of 361 merged cell types.

Our merging strategy groups biosamples by their annotated cell type (biosample term name in ENCODE) and further organizes them based on isogenic replicate relationships. Within each cell type, we identify sets of biosamples that are isogenic replicates (derived from the same donor) and share at least three assays in common---these form replicate groups. Each replicate group is assigned a group identifier (grp), and individual biosamples within a group are labeled as replicates (rep). For example, \texttt{cardiac\_muscle\_cell\_grp1\_rep1} and \texttt{cardiac\_muscle\_cell\_grp1\_rep2} represent two isogenic replicates from the same cardiac muscle cell type that share a common set of experiments. Biosamples that lack isogenic replicate annotations or do not share sufficient experiments with other samples are aggregated into a single ``non-replicate'' sample (e.g., \texttt{right\_lobe\_of\_liver\_nonrep}), where experiments are selected to maximize consistency of donor, lab, and biosample accession. This strategy preserves biological relationships while maximizing the number of assays available per merged sample for training.

\subsubsection{Control Integration}

For each biosample containing ChIP-seq experiments, we obtained the corresponding ChIP-seq control (input) experiments. These control signals are provided as an additional always-available input channel to the model, forming an $(A+1)$-dimensional input where the $(A+1)$-th feature represents the control signal. Controls are never masked during training, serving as a stable reference for batch effect normalization.

\subsubsection{Downsampling}

To simulate varying sequencing depths, we implemented read downsampling in BAM files. The Downsampling Factor (DSF) determines the fraction of reads retained: $\text{DSF} = 1$ retains all reads, $\text{DSF} = 2$ randomly samples 50\% of reads, and $\text{DSF} = 4$ randomly samples 25\% of reads. The default DSF list used during training is $\{1, 2, 4\}$.


\subsection{CANDI Architecture}

Let $A = 35$ represent the number of assay types, and let $G = 30{,}000$ bp and $R = 25$ bp denote the length of the genomic window and the resolution, respectively, such that the window contains $L = G / R = 1{,}200$ genomic positions at 25 bp resolution. Each sample (i.e., one data instance in a batch) corresponds to epigenomic data from a single cell type, forming a matrix $\mathbf{M}$ of shape $L \times A$ for that cell type. We process multiple samples in a batch of size $N$, where each sample has its own $\mathbf{M}$, $\mathbf{S}$, and covariates.

The model receives:
\begin{itemize}
    \item $\mathbf{M} \in \mathbb{R}^{N \times L \times (A+1)}$ --- Epigenomic read counts including control channel
    \item $\mathbf{S} \in \{0,1\}^{N \times 4 \times G}$ --- One-hot encoded DNA sequence
    \item $\mathbf{C}_{\text{in}} \in \mathbb{R}^{N \times 4 \times (A+1)}$ --- Input experimental covariates
    \item $\mathbf{C}_{\text{out}} \in \mathbb{R}^{N \times 4 \times A}$ --- Output experimental covariates
\end{itemize}

\subsubsection{Inputs}

\paragraph{Epigenomic Reads $\mathbf{M}$:} Each $M_{\ell,a}$ is the read count for assay $a$ at position $\ell$. The $(A+1)$-th channel contains ChIP-seq control signal. Input counts are transformed using $\text{arcsinh}(x)$ before being fed to the encoder, which stabilizes variance across the dynamic range of read counts.

\paragraph{DNA Sequence $\mathbf{S}$:} One-hot encoded nucleotides (A, C, G, T) across the $G = 30{,}000$ bp region.

\paragraph{Covariates $\mathbf{C}_{\text{in}}$ and $\mathbf{C}_{\text{out}}$:} Four values per assay: $\log_2(\text{sequencing\_depth})$, run\_type (single/paired-end), read\_length, and sequencing\_platform.

\subsubsection{Outputs}

For each assay $a$ at each position $\ell$, the model predicts parameters of probability distributions:
\begin{align}
    (\hat{n}_{a,\ell}, \hat{p}_{a,\ell}) &\in \mathbb{R}^{N \times L \times A} \quad \text{--- Negative Binomial parameters for raw counts} \\
    (\hat{\mu}_{a,\ell}, \hat{\sigma}^2_{a,\ell}) &\in \mathbb{R}^{N \times L \times A} \quad \text{--- Gaussian parameters for signal } p\text{-values} \\
    \text{peak}_{a,\ell} &\in [0,1]^{N \times L \times A} \quad \text{--- Peak probability scores}
\end{align}

\paragraph{Choice of Probability Distributions.}
The choice of probability distributions is motivated by the statistical properties of each output type. For raw read counts, we use the negative binomial distribution, which is well-established for modeling sequencing count data due to its ability to capture overdispersion---a common characteristic of epigenomic experiments where variance exceeds the mean. The Poisson distribution is a special case of the negative binomial with fixed dispersion, but empirical studies have shown that epigenomic count data typically exhibit overdispersion, making the negative binomial more appropriate. 

For processed signal values, we predict $\text{arcsinh}$-transformed $p$-values following previous methods for variance stabilization. We model these transformed values with a Gaussian distribution, predicting both the mean ($\mu$) and variance ($\sigma^2$). Unlike standard MSE optimization, which implicitly assumes constant variance across all predictions, our approach allows the model to express heteroscedastic (position-varying) uncertainty. By jointly optimizing both parameters via Gaussian negative log-likelihood, the model learns to output higher variance in regions where predictions are inherently more uncertain---capturing aleatoric uncertainty that reflects irreducible noise in the data rather than model limitations.

\subsubsection{Model Overview}

The model consists of an encoder $\mathcal{E}$ and three separate decoders $\mathcal{D}_{\text{count}}$, $\mathcal{D}_{\text{signal}}$, $\mathcal{D}_{\text{peak}}$:
\begin{align}
    \mathcal{E}(\mathbf{M}, \mathbf{S}, \mathbf{C}_{\text{in}}) &\rightarrow \mathbf{Z} \\
    \mathcal{D}_{\text{count}}(\mathbf{Z}, \mathbf{C}_{\text{out}}) &\rightarrow (\hat{n}, \hat{p}) \\
    \mathcal{D}_{\text{signal}}(\mathbf{Z}, \mathbf{C}_{\text{out}}) &\rightarrow (\hat{\mu}, \hat{\sigma}^2) \\
    \mathcal{D}_{\text{peak}}(\mathbf{Z}, \mathbf{C}_{\text{out}}) &\rightarrow \text{peak}
\end{align}

\subsubsection{Encoder $\mathcal{E}$}

The encoder integrates epigenomic data $\mathbf{M}$, DNA sequence context $\mathbf{S}$, and experimental covariates $\mathbf{C}_{\text{in}}$ into a latent representation $\mathbf{Z} \in \mathbb{R}^{N \times L' \times d_{\text{model}}}$.

\paragraph{1. Epigenetic Signal Convolution Tower:} 
Processes the epigenomic signals with $n_{\text{cnn}} = 3$ depth-wise separable convolution layers. Each layer applies convolution with kernel size $k = 3$, followed by batch normalization, GELU activation, and average pooling with $\text{pool\_size} = 2$. This progressively reduces the spatial dimension from $L = 1{,}200$ to $L' = 150$ while expanding the feature dimension by $\text{expansion\_factor} = 3$ at each layer:
\begin{equation}
    L' = \frac{L}{\text{pool\_size}^{n_{\text{cnn}}}} = \frac{1200}{2^3} = 150
\end{equation}
After each convolution layer, a MetadataCrossAttention module conditions the features on experimental covariates using FiLM (Feature-wise Linear Modulation).

\paragraph{2. DNA Sequence Convolution Tower:} 
Processes the one-hot encoded DNA sequence with $n_{\text{cnn}} + 2 = 5$ convolution layers. The first three layers use $\text{pool\_size} = 2$, and the final two layers use $\text{pool\_size} = 5$ to match the epigenomic resolution:
\begin{equation}
    L'_{\text{DNA}} = \frac{G}{\text{pool\_size}^{n_{\text{cnn}}} \times 5^2} = \frac{30000}{2^3 \times 25} = 150
\end{equation}
This produces DNA features $\mathbf{S}^{(\text{final})} \in \mathbb{R}^{N \times L' \times d_{\text{model}}}$.

\paragraph{3. Metadata Cross-Attention:} 
At each convolutional layer, experimental covariates modulate the learned features through cross-attention. For each assay $a$, we embed the four covariates (depth, run\_type, read\_length, sequencing\_platform) into a query vector $\mathbf{q}_a$. This query attends to the spatial features of that assay, producing scale ($\boldsymbol{\gamma}_a$) and shift ($\boldsymbol{\beta}_a$) parameters for FiLM conditioning:
\begin{equation}
    \mathbf{h}'_a = \boldsymbol{\gamma}_a \odot \mathbf{h}_a + \boldsymbol{\beta}_a
\end{equation}
where $\odot$ denotes element-wise multiplication and $\mathbf{h}_a$ are the features for assay $a$.

\paragraph{4. Fusion:} 
The epigenomic and DNA features are concatenated along the feature dimension and projected back to $d_{\text{model}}$ dimensions through a linear layer with layer normalization:
\begin{equation}
    \mathbf{H}^{(0)} = \text{LayerNorm}\left(\mathbf{W}_{\text{fusion}} \left[\mathbf{M}^{(\text{final})}; \mathbf{S}^{(\text{final})}\right] + \mathbf{b}_{\text{fusion}}\right)
\end{equation}

\paragraph{5. Transformer Encoder:} 
Applies $n_{\text{sab}} = 4$ transformer encoder layers with $n_{\text{head}} = 9$ attention heads. We use rotary positional embeddings (RoPE) for improved length generalization. Each layer consists of multi-head self-attention followed by a feed-forward network with GELU activation and dropout $= 0.1$:
\begin{align}
    \mathbf{H}^{(i+1)} &= \text{FFN}\left(\text{MultiHeadAttn}\left(\mathbf{H}^{(i)}\right)\right) \\
    \mathbf{Z} &= \mathbf{H}^{(n_{\text{sab}})}
\end{align}

\subsubsection{Decoders $\mathcal{D}$}

Given latent representation $\mathbf{Z}$ and output covariates $\mathbf{C}_{\text{out}}$, three separate decoders reconstruct the output distributions:

\paragraph{1. Output Covariate Embedding:} 
Output covariates are embedded using the same MetadataCrossAttention mechanism as the encoder, providing the model with information about the desired output experimental conditions.

\paragraph{2. Deconvolution Towers:} 
Each decoder employs $n_{\text{cnn}} = 3$ transposed convolution layers that invert the encoder's pooling and feature compression. After each deconvolution layer, MetadataCrossAttention conditions the features on output covariates, allowing the model to generate outputs appropriate for specific experimental conditions.

\paragraph{3. Distribution Output Layers:}
\begin{itemize}
    \item \textbf{NegativeBinomialLayer:} Two linear projections produce $\hat{n}$ (via softplus for positivity) and $\hat{p}$ (via sigmoid for $[0,1]$ range):
    \begin{align}
        \hat{n} &= \text{softplus}(\mathbf{W}_n \mathbf{x} + \mathbf{b}_n) \\
        \hat{p} &= \sigma(\mathbf{W}_p \mathbf{x} + \mathbf{b}_p)
    \end{align}
    
    \item \textbf{GaussianLayer:} Two linear projections produce $\hat{\mu}$ (via softplus) and $\hat{\sigma}^2$ (via softplus):
    \begin{align}
        \hat{\mu} &= \text{softplus}(\mathbf{W}_\mu \mathbf{x} + \mathbf{b}_\mu) \\
        \hat{\sigma}^2 &= \text{softplus}(\mathbf{W}_\sigma \mathbf{x} + \mathbf{b}_\sigma)
    \end{align}
    
    \item \textbf{PeakLayer:} A linear projection followed by sigmoid produces peak probabilities in $[0,1]$:
    \begin{equation}
        \text{peak} = \sigma(\mathbf{W}_{\text{peak}} \mathbf{x} + \mathbf{b}_{\text{peak}})
    \end{equation}
\end{itemize}


\subsection{Learning Objectives and Training Strategy}

Our approach leverages self-supervised learning, where the model learns to reconstruct deliberately corrupted versions of its own input. We employ three complementary masking strategies that can be combined with configurable probabilities.

\subsubsection{Full Assay Masking}

We randomly mask $k$ complete assays for each cell type, where $k \sim \text{Uniform}(1, n_{\text{available}} - 1)$. Both the data AND metadata for selected assays are masked, simulating the imputation scenario where some experiments are entirely missing. At least one assay always remains available per cell type to provide context.

\subsubsection{Full Loci Masking}

We randomly select contiguous chunks of genomic positions (default $\text{chunk\_size} = 40$ positions, $\sim$1kb) and mask these positions across ALL available assays simultaneously. This objective most closely parallels BERT-style masked language modeling, where the model learns to predict masked tokens from surrounding context. Here, genomic positions serve as ``tokens,'' and the model must infer epigenomic signals at masked positions using flanking regions and DNA sequence.

\subsubsection{Denoising via Upsampling}

For assays that remain unmasked, the model is trained to reconstruct the original high-quality signal from potentially downsampled (noisy) observations. The observed regions provide the upsampling/denoising training signal.

\subsubsection{Data Augmentation}

We apply reverse complement augmentation with probability 0.5. When applied, both the DNA sequence and epigenomic signals are reversed along the genomic axis, and the DNA bases are complemented (A$\leftrightarrow$T, C$\leftrightarrow$G). This data augmentation helps the model learn strand-invariant representations.

\subsubsection{Loss Function}

We train CANDI by minimizing a weighted combination of negative log-likelihood losses:
\begin{equation}
    \mathcal{L} = w_{\text{obs}} \cdot \mathcal{L}_{\text{obs}} + w_{\text{imp}} \cdot \mathcal{L}_{\text{imp}}
\end{equation}
where:
\begin{align}
    \mathcal{L}_{\text{obs}} &= w_{\text{count}} \cdot \mathcal{L}_{\text{count,obs}} + w_{\text{pval}} \cdot \mathcal{L}_{\text{pval,obs}} + w_{\text{peak}} \cdot \mathcal{L}_{\text{peak,obs}} \\
    \mathcal{L}_{\text{imp}} &= w_{\text{count}} \cdot \mathcal{L}_{\text{count,imp}} + w_{\text{pval}} \cdot \mathcal{L}_{\text{pval,imp}} + w_{\text{peak}} \cdot \mathcal{L}_{\text{peak,imp}}
\end{align}

The individual loss components are:
\begin{align}
    \mathcal{L}_{\text{count}} &= -\sum_{(a,\ell) \in \mathcal{S}} \log P_{\text{NB}}\left(y_{a,\ell}^{\text{count}} \mid \hat{n}_{a,\ell}, \hat{p}_{a,\ell}\right) \\
    \mathcal{L}_{\text{pval}} &= -\sum_{(a,\ell) \in \mathcal{S}} \log P_{\mathcal{N}}\left(y_{a,\ell}^{\text{signal}} \mid \hat{\mu}_{a,\ell}, \hat{\sigma}^2_{a,\ell}\right) \\
    \mathcal{L}_{\text{peak}} &= -\sum_{(a,\ell) \in \mathcal{S}} \left[ y_{a,\ell}^{\text{peak}} \log(\text{peak}_{a,\ell}) + (1 - y_{a,\ell}^{\text{peak}}) \log(1 - \text{peak}_{a,\ell}) \right]
\end{align}

where $\mathcal{S}$ denotes the set of observed or imputed positions depending on the subscript.

The negative binomial probability mass function is:
\begin{equation}
    P_{\text{NB}}(k \mid n, p) = \binom{k + n - 1}{k} p^n (1-p)^k
\end{equation}

The Gaussian probability density function is:
\begin{equation}
    P_{\mathcal{N}}(y \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y - \mu)^2}{2\sigma^2}\right)
\end{equation}

Default loss weights: $w_{\text{count}} = 0.5$, $w_{\text{pval}} = 1.0$, $w_{\text{peak}} = 0.1$, $w_{\text{obs}} = 0.25$, $w_{\text{imp}} = 1.0$.

\subsubsection{Training Loci}

We use multiple strategies for selecting training genomic loci:
\begin{itemize}
    \item \textbf{cCRE (default):} Focus on genomic regions containing candidate cis-regulatory elements, which are enriched with informative epigenomic signals
    \item \textbf{random:} Randomly selected non-overlapping regions across the genome
    \item \textbf{full\_chr:} Complete coverage of specified chromosomes
    \item \textbf{gw (genome-wide):} Full coverage across all autosomes and sex chromosomes
\end{itemize}

For all strategies, we exclude ENCODE blacklist regions (known problematic regions with anomalous signal accumulation) to ensure training data quality.

We trained CANDI on 5,000 randomly selected, non-overlapping regions, each spanning 30kb. We excluded chromosome 21, reserving it exclusively for testing purposes. In total, CANDI was trained on 150 million base pairs of genomic sequence, representing approximately 5\% of the human genome.

\subsubsection{Model Hyperparameters}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Context length ($L$) & 1,200 bins (30,000 bp) \\
Resolution ($R$) & 25 bp \\
Number of assays ($A$) & 35 \\
CNN layers ($n_{\text{cnn}}$) & 3 \\
Transformer layers ($n_{\text{sab}}$) & 4 \\
Attention heads ($n_{\text{head}}$) & 9 \\
Expansion factor & 3 \\
Pool size & 2 \\
Kernel size & 3 \\
Dropout & 0.1 \\
Optimizer & Adamax \\
Learning rate & $5 \times 10^{-4}$ \\
Batch size & 90 \\
\bottomrule
\end{tabular}
\caption{CANDI model hyperparameters.}
\label{tab:hyperparams}
\end{table}


\subsection{Evaluation Metrics}

\subsubsection{Imputation Performance}

To evaluate imputation performance, we compute the following genome-wide metrics: Mean Squared Error (MSE) for average squared differences, Pearson Correlation for linear relationships, Spearman Correlation for monotonic relationships, and $R^2$ score between observed and imputed values. The main CANDI model is trained on the Extended ENCODE Dataset, but to ensure comparability with the ENCODE Imputation Challenge (EIC) competitors, we trained a version of CANDI using the EIC dataset and their specified train-test split. All performance evaluations were made on chromosome 21.

\subsubsection{Peak Prediction Performance}

For peak prediction, we evaluate using standard binary classification metrics: precision, recall, and area under the ROC curve (AUROC). We compare predicted peak probabilities against observed peak calls from ENCODE narrowPeak files.

\subsubsection{Uncertainty Modeling}

\paragraph{Confidence Calibration.}
To evaluate the reliability of the model's uncertainty estimates, we assess its calibration for all three output types. For counts and signal values, we measure the fraction of observed values that fall within the model's predicted $C\%$ confidence interval for $C \in [0, 1]$. For negative binomial predictions, confidence intervals are computed from the predicted $(n, p)$ parameters. For Gaussian predictions, intervals are computed as $\mu \pm z_{C/2} \times \sigma$, where $z_{C/2}$ is the critical value of the standard normal distribution. A well-calibrated model should exhibit a linear relationship between expected and observed coverage.

We visualize calibration using calibration curves, which plot the empirical coverage (fraction of observations within interval) against the nominal confidence level. For each confidence level $c \in [0, 1]$, we compute the $c\%$ confidence interval from the predicted distribution and measure what fraction of true observations fall within this interval. A perfectly calibrated model produces a diagonal line ($y = x$), indicating that $c\%$ of observations fall within the $c\%$ confidence interval.

\paragraph{Concordance Index.}
Beyond interval coverage, we evaluate whether the model's uncertainty estimates correctly rank predictions using the concordance index (C-index). For Gaussian predictions with mean $\mu$ and standard deviation $\sigma$, the C-index measures the probability that the model correctly ranks pairs of observations by their true values. Formally, for a pair of positions $(i, j)$, we compute the probability that observation $i$ exceeds observation $j$ under the model:
\begin{equation}
    P(Y_i > Y_j) = \Phi\left(\frac{\mu_i - \mu_j}{\sqrt{\sigma_i^2 + \sigma_j^2}}\right)
\end{equation}
where $\Phi$ is the standard normal CDF. The C-index is then the AUC-ROC score comparing these predicted probabilities against the true binary labels ($\mathbf{1}[y_i > y_j]$). A C-index of 0.5 indicates random ranking, while 1.0 indicates perfect discrimination. This metric assesses whether the model's distributional predictions meaningfully capture the relative ordering of true signal values.


\subsection{RNA-seq Prediction as Biological Validation}

To validate that CANDI captures biologically meaningful information beyond simple signal reconstruction, we evaluate how well epigenomic features derived from the model's predictions can predict gene expression levels measured by RNA-seq. Importantly, CANDI has never seen any RNA-seq data during training---this evaluation tests whether the model's learned representations of epigenomic signals implicitly encode transcriptional regulatory information.

For each gene, we extract summary features from epigenomic signals around three key regions: the transcription start site (TSS), gene body, and transcription end site (TES). We use adaptive margins set to 10\% of gene length for TSS and TES regions. From each region, we compute summary statistics including the median, interquartile range, minimum, and maximum signal values. We extract features from four signal sources:
\begin{enumerate}
    \item Observed epigenomic signals from available assays only
    \item Denoised signals for assays where observations were provided as input
    \item Denoised+imputed signals across all 35 assays---combining denoised predictions for available assays with imputed predictions for missing assays
    \item Latent representations $\mathbf{Z}$ extracted from CANDI's transformer encoder
\end{enumerate}

For the latent representations, we compute the same summary statistics across each latent dimension for all three gene regions.

We employ nested cross-validation to rigorously evaluate predictive performance while avoiding overfitting. The outer loop uses 5-fold cross-validation to estimate generalization performance, while the inner loop performs hyperparameter selection via grid search with 4-fold cross-validation. Each fold trains a pipeline consisting of standardization, dimensionality reduction (PCA retaining 50--90\% variance), and a regression model. We evaluate ridge and lasso regression, reporting Pearson correlation, Spearman correlation, and $R^2$ between predicted and observed $\log$-transformed TPM values.

If denoised signals outperform observed signals, this indicates the model successfully removes noise while preserving biologically relevant information. If denoised+imputed signals further improve predictions, this demonstrates that imputation adds meaningful regulatory context by filling in missing assays with biologically accurate predictions. The latent embeddings $\mathbf{Z}$ represent a rich, compressed representation of the epigenomic state that may contain biologically relevant information not explicitly decoded into signal predictions but potentially informative for expression prediction.

